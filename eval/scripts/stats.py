"""
Collects structural stats on dataflow graphs generated by Wavelet and RipTide. 
"""

from dataclasses import dataclass

import os
import json
import argparse

import networkx as nx
from networkx.drawing.nx_pydot import write_dot

import riptide

BENCH_NAMES = [
    "nn_vadd",
    "nn_norm",
    "nn_relu",
    "nn_pool",
    "nn_fc",
    "nn_conv",
    "dmv",
    "dmm",
    "dither",
    "sort",
    # "dconv",
]

WAVELET_NAME_FORMAT = lambda name: f"{name}.dfg.json"
RIPTIDE_NAME_FORMAT = lambda name: f"{name}.{name}.o2p"

def parse_wavelet_op(op) -> str:
    if isinstance(op, str):
        return op
    elif isinstance(op, dict):
        keys = list(op.keys())
        assert len(keys) == 1, f"unexpected op format {op}"
        if keys[0] == "merge" and op["merge"]["state"] == "popLeft":
            return "carry"
        return keys[0]
    else:
        assert False, f"unexpected op format {op}"

def bypass_node(G: nx.DiGraph, v):
    """
    Remove node `v` from DiGraph G while connecting all predecessors -> successors.
    """
    preds = list(G.predecessors(v))
    succs = list(G.successors(v))
    for w in preds:
        for u in succs:
            if not G.has_edge(w, u):
                G.add_edge(w, u)
    G.remove_node(v)
    return G

def bypass_all_cf(G: nx.DiGraph):
    for v in list(G.nodes):
        if G.nodes[v]["op_kind"] != "mem":
            bypass_node(G, v)

def wavelet_op_type(op_name: str) -> str:
    if op_name in [
        "add", "sub", "mul", "div", "shl", "ashr", "lshr", "sel",
        "bitand", "eq", "lt", "le", "neq", "and", "const", "copy"]:
        return "sync"
    elif op_name in ["load", "store"]:
        return "mem"
    elif op_name in [
        "switch", "steer", "merge", "carry", "forward", "fork", "order",
        "forwardc", "sink", "inv", "inact",
    ]:
        return "cf"
    else:
        assert False, f"unknown op name {op_name}"

def riptide_op_type(op_name: str) -> str:
    if op_name.startswith("ARITH_") or op_name.startswith("MUL_"):
        return "sync"
    elif op_name.startswith("MEM_"):
        return "mem"
    elif op_name.startswith("CF_"):
        return "cf"
    else:
        assert False, f"unknown op name {op_name}"

def load_wavelet_dfg(path: str) -> nx.DiGraph:
    with open(path, "r") as f:
        dfg = json.load(f)

        chans = set()
        edge_to_node = {}
        edge_from_node = {}

        nx_dfg = nx.DiGraph()
        
        for idx, atom in enumerate(dfg["atoms"]):
            if "async" in atom:
                atom = atom["async"]
            elif "op" in atom:
                atom = atom["op"]
            else:
                assert False, f"unknown atom {atom}"

            op_name = parse_wavelet_op(atom["op"])
            op_kind = wavelet_op_type(op_name)
            nx_dfg.add_node(idx, op_name=op_name, op_kind=op_kind)

            for i, chan in enumerate(atom["inputs"]):
                # if op_names[idx] == "carry" and (i == 0 or i == 2):
                #     # Not including carry back edges
                #     continue
                assert chan not in edge_to_node, f"input {chan} used multiple times"
                edge_to_node[chan] = idx
                chans.add(chan)

            for chan in atom["outputs"]:
                assert chan not in edge_from_node, f"output {chan} used multiple times"
                edge_from_node[chan] = idx
                chans.add(chan)

        for chan in chans:
            if chan in edge_to_node and chan in edge_from_node:
                src = edge_from_node[chan]
                dst = edge_to_node[chan]
                nx_dfg.add_edge(src, dst)

        # Bypass every fork, forward, and const node
        for v in list(nx_dfg.nodes):
            if nx_dfg.nodes[v]["op_name"] in ["fork", "forward", "const"]:
                bypass_node(nx_dfg, v)

        return nx_dfg

def load_riptide_dfg(path: str) -> nx.DiGraph:
    with open(path, "r") as f:
        dfg = json.load(f)
        dfg = riptide.DataflowGraph.load_dataflow_graph(dfg)

        nx_dfg = nx.DiGraph()

        for op in dfg.vertices:
            nx_dfg.add_node(op.id)
            nx_dfg.nodes[op.id]["op_name"] = op.operator
            nx_dfg.nodes[op.id]["op_kind"] = riptide_op_type(op.operator)

        for channel in dfg.channels:
            if channel.source is not None and channel.destination is not None:
                # op = dfg.vertices[channel.destination]
                # if op.operator == "CF_CFG_OP_CARRY" and channel.destination_port in [0, 2]:
                #     print(channel.destination_port)
                #     # Not including carry back edges
                #     continue

                nx_dfg.add_edge(channel.source, channel.destination)

        return nx_dfg

@dataclass
class Stats:
    total_ops: int
    num_sync_ops: int
    num_cf_ops: int
    num_mem_ops: int
    num_orders: int
    num_carrys: int
    cf_overhead: float
    num_sccs: int

    def print(self):
        print(f"total ops: {self.total_ops}")
        print(f"num sync ops: {self.num_sync_ops}")
        print(f"num cf ops: {self.num_cf_ops}")
        print(f"num mem ops: {self.num_mem_ops}")
        print(f"num orders: {self.num_orders}")
        print(f"num carrys: {self.num_carrys}")
        print(f"cf overhead: {self.cf_overhead:.2f}")
        print(f"num sccs: {self.num_sccs}")

def collect_common_stats(G: nx.DiGraph) -> Stats:
    total_ops = G.number_of_nodes()
    num_sync_ops = sum(1 for _, data in G.nodes(data=True) if data["op_kind"] == "sync")
    num_cf_ops = sum(1 for _, data in G.nodes(data=True) if data["op_kind"] == "cf")
    num_mem_ops = sum(1 for _, data in G.nodes(data=True) if data["op_kind"] == "mem")
    num_orders = sum(1 for _, data in G.nodes(data=True) if data["op_name"] == "order" or data["op_name"] == "CF_CFG_OP_ORDER")
    num_carrys = sum(1 for _, data in G.nodes(data=True) if data["op_name"] == "carry" or data["op_name"] == "CF_CFG_OP_CARRY")
    cf_overhead = num_cf_ops / (num_mem_ops + num_sync_ops)
    
    # bypass_all_cf(G)
    num_sccs = len(list(nx.strongly_connected_components(G)))

    # G = nx.condensation(wavelet_dfg)
    # for n, data in G.nodes(data=True):
    #     G.nodes[n]["label"] = str(len(data["members"]))
    #     G.nodes[n]["ops"] = ", ".join([wavelet_dfg.nodes[m]["op_name"] for m in data["members"]])
    # write_dot(G, "graph-wavelet.dot")

    return Stats(
        total_ops=total_ops,
        num_sync_ops=num_sync_ops,
        num_cf_ops=num_cf_ops,
        num_mem_ops=num_mem_ops,
        num_orders=num_orders,
        num_carrys=num_carrys,
        cf_overhead=cf_overhead,
        num_sccs=num_sccs,
    )

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("wavelet_dir", help="Directory containing Wavelet-generated dataflow graphs")
    parser.add_argument("riptide1_dir", help="Directory containing RipTide-generated dataflow graphs (config 1)")
    parser.add_argument("riptide2_dir", help="Directory containing RipTide-generated dataflow graphs (config 2)")
    args = parser.parse_args()

    stats = {}

    for bench in BENCH_NAMES:
        wavelet_path = os.path.join(args.wavelet_dir, WAVELET_NAME_FORMAT(bench))
        riptide1_path = os.path.join(args.riptide1_dir, RIPTIDE_NAME_FORMAT(bench))
        riptide2_path = os.path.join(args.riptide2_dir, RIPTIDE_NAME_FORMAT(bench))

        print(f"================ analyzing {bench} ================")

        # with open(wavelet_path, "r") as wavelet_dfg, \
        #     open(riptide_path, "r") as riptide_dfg:
        wavelet_dfg = load_wavelet_dfg(wavelet_path)
        riptide1_dfg = load_riptide_dfg(riptide1_path)
        riptide2_dfg = load_riptide_dfg(riptide2_path)

        wavelet_stats = collect_common_stats(wavelet_dfg)
        riptide1_stats = collect_common_stats(riptide1_dfg)
        riptide2_stats = collect_common_stats(riptide2_dfg)

        stats[bench] = {
            "wavelet": wavelet_stats,
            "riptide1": riptide1_stats,
            "riptide2": riptide2_stats,
        }

        print("---- Wavelet")
        wavelet_stats.print()
        print("---- RipTide 1")
        riptide1_stats.print()
        print("---- RipTide 2")
        riptide2_stats.print()

    # Plot the final LaTeX tabular
    # total_ops | num_sync_ops | num_cf_ops | cf_overhead

    compilers = [
        ("W", "wavelet"),
        ("R", "riptide1"),
        # ("R$_2$", "riptide2"),
    ]

    metrics = [
        ("\\#Ops", lambda s: s.total_ops),
        # ("\\#Sync", lambda s: s.num_sync_ops),
        ("\\#CF", lambda s: s.num_cf_ops),
        ("Overhead", lambda s: f"{s.cf_overhead:.2f}"),
        # ("SCCs", lambda s: s.num_sccs),
    ]

    print("\\begin{tabular}{l" + (("|" + "r" * len(compilers)) * len(metrics)) + "|r}")

    print("".join([
        f" & \\multicolumn{{{len(compilers)}}}{{{'c|' if i + 1 == len(metrics) else 'c|'}}}{{{header}}}"
        for i, (header, _) in enumerate(metrics)
    ]) + " \\\\")
    print("Test" + "".join([f" & {header}" for header, _ in compilers] * len(metrics)) + " & W/R \\\\")
    print("\\hline")

    for bench in BENCH_NAMES:
        columns = [
            f"\\texttt{{{bench.replace('_', '\\textunderscore{}')}}}"
        ]

        for _, metric_fn in metrics:
            for _, name in compilers:
                columns.append(str(metric_fn(stats[bench][name])))

        print(" & ".join(columns) + f" & {stats[bench]['wavelet'].total_ops / stats[bench]['riptide1'].total_ops:.2f} \\\\")

    print("\\end{tabular}")

if __name__ == "__main__":
    main()
